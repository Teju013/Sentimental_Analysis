{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 9891389,
          "sourceType": "datasetVersion",
          "datasetId": 6074826
        }
      ],
      "dockerImageVersionId": 30786,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "distilbert_new",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "import kagglehub\n",
        "tejaswiniramoju_datasetlargeone_path = kagglehub.dataset_download('tejaswiniramoju/datasetlargeone')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "ETXO0-Omukp5"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:39:19.425582Z",
          "iopub.execute_input": "2024-11-13T10:39:19.42608Z",
          "iopub.status.idle": "2024-11-13T10:39:19.44188Z",
          "shell.execute_reply.started": "2024-11-13T10:39:19.426037Z",
          "shell.execute_reply": "2024-11-13T10:39:19.441002Z"
        },
        "id": "ihqnaftbukp6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pandas torch\n",
        "!pip install datasets"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:39:19.443399Z",
          "iopub.execute_input": "2024-11-13T10:39:19.443708Z",
          "iopub.status.idle": "2024-11-13T10:39:43.619736Z",
          "shell.execute_reply.started": "2024-11-13T10:39:19.443677Z",
          "shell.execute_reply": "2024-11-13T10:39:43.618559Z"
        },
        "id": "mw1Pm3QNukp7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Change the file path to the Kaggle input directory\n",
        "dataset = pd.read_csv(\"/kaggle/input/datasetlargeone/EMOTION_LARGE.csv\")\n",
        "dataset.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:39:43.62365Z",
          "iopub.execute_input": "2024-11-13T10:39:43.624022Z",
          "iopub.status.idle": "2024-11-13T10:39:43.678668Z",
          "shell.execute_reply.started": "2024-11-13T10:39:43.623979Z",
          "shell.execute_reply": "2024-11-13T10:39:43.677829Z"
        },
        "id": "8c4KL1GBukp7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "label_mapping = {\n",
        "    'sadness': 0,\n",
        "    'joy': 1,\n",
        "    'love': 2,\n",
        "    'angry': 3,\n",
        "    'fear': 4,\n",
        "    'surprise': 5\n",
        "}\n",
        "\n",
        "# First, split the dataset into training (70%) and temporary (30%) sets\n",
        "train_df, temp_df = train_test_split(dataset, test_size=0.3, random_state=42)  # 70% train, 30% temp\n",
        "\n",
        "# Then, split the temporary set into validation (20%) and test (10%) sets\n",
        "# To achieve this, we need 2/3 of the temp_df for validation and 1/3 for testing\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.3333, random_state=42)  # 1/3 for test, 2/3 for validation\n",
        "\n",
        "# Now you have:\n",
        "# - train_df: 70% of the original dataset\n",
        "# - val_df: 20% of the original dataset\n",
        "# - test_df: 10% of the original datas"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:39:43.680867Z",
          "iopub.execute_input": "2024-11-13T10:39:43.681175Z",
          "iopub.status.idle": "2024-11-13T10:39:43.826833Z",
          "shell.execute_reply.started": "2024-11-13T10:39:43.681143Z",
          "shell.execute_reply": "2024-11-13T10:39:43.82607Z"
        },
        "id": "MXpRndefukp7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
        "from datasets import Dataset\n",
        "\n",
        "# Convert dataframes into Hugging Face datasets\n",
        "hf_train_dataset = Dataset.from_pandas(train_df)\n",
        "hf_val_dataset = Dataset.from_pandas(val_df)\n",
        "hf_test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Define the label mapping based on dataset's labels\n",
        "label_mapping = {\n",
        "    'sadness': 0,\n",
        "    'joy': 1,\n",
        "    'love': 2,\n",
        "    'anger': 3,\n",
        "    'fear': 4,\n",
        "    'surprise': 5\n",
        "}\n",
        "\n",
        "# Map the labels to integers in the datasets\n",
        "hf_train_dataset = hf_train_dataset.map(lambda examples: {'label': label_mapping[examples['labels']]})\n",
        "hf_val_dataset = hf_val_dataset.map(lambda examples: {'label': label_mapping[examples['labels']]})\n",
        "hf_test_dataset = hf_test_dataset.map(lambda examples: {'label': label_mapping[examples['labels']]})\n",
        "\n",
        "# Load the DistilBERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-cased\")\n",
        "\n",
        "# Tokenization function for dataset\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# Tokenize all datasets\n",
        "tokenized_train_dataset = hf_train_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_val_dataset = hf_val_dataset.map(tokenize_function, batched=True)\n",
        "tokenized_test_dataset = hf_test_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:39:43.828101Z",
          "iopub.execute_input": "2024-11-13T10:39:43.828452Z",
          "iopub.status.idle": "2024-11-13T10:39:51.387986Z",
          "shell.execute_reply.started": "2024-11-13T10:39:43.828403Z",
          "shell.execute_reply": "2024-11-13T10:39:51.387104Z"
        },
        "id": "7nHtVeIfukp7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the tokenized datasets\n",
        "train_dataset = tokenized_train_dataset.shuffle(seed=42)  # Shuffle the training dataset\n",
        "val_dataset = tokenized_val_dataset.shuffle(seed=42)      # Shuffle the validation dataset\n",
        "eval_dataset = tokenized_test_dataset.shuffle(seed=42)    # Shuffle the testing dataset\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Convert the shuffled datasets to pandas DataFrames\n",
        "train_df = pd.DataFrame(train_dataset)  # Convert the shuffled training dataset to DataFrame\n",
        "val_df = pd.DataFrame(val_dataset)      # Convert the shuffled validation dataset to DataFrame\n",
        "eval_df = pd.DataFrame(eval_dataset)    # Convert the shuffled evaluation dataset to DataFrame\n",
        "\n",
        "# Save the DataFrames to CSV files\n",
        "train_df.to_csv('train_dataset.csv', index=False)  # Save training DataFrame as CSV\n",
        "val_df.to_csv('val_dataset.csv', index=False)      # Save validation DataFrame as CSV\n",
        "eval_df.to_csv('eval_dataset.csv', index=False)    # Save evaluation DataFrame as CSV"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:39:51.389169Z",
          "iopub.execute_input": "2024-11-13T10:39:51.389474Z",
          "iopub.status.idle": "2024-11-13T10:40:10.338185Z",
          "shell.execute_reply.started": "2024-11-13T10:39:51.389441Z",
          "shell.execute_reply": "2024-11-13T10:40:10.337091Z"
        },
        "id": "OwFayP4dukp7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=len(label_mapping))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:40:10.339828Z",
          "iopub.execute_input": "2024-11-13T10:40:10.340225Z",
          "iopub.status.idle": "2024-11-13T10:40:10.475693Z",
          "shell.execute_reply.started": "2024-11-13T10:40:10.340185Z",
          "shell.execute_reply": "2024-11-13T10:40:10.47452Z"
        },
        "id": "q1pWy-oMukp8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install evaluate\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:40:10.477071Z",
          "iopub.execute_input": "2024-11-13T10:40:10.477687Z",
          "iopub.status.idle": "2024-11-13T10:40:22.437116Z",
          "shell.execute_reply.started": "2024-11-13T10:40:10.477641Z",
          "shell.execute_reply": "2024-11-13T10:40:22.43598Z"
        },
        "id": "HTKL0Rkyukp8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import numpy as np\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "# Calculate class weights based on label distribution in the training data\n",
        "labels = train_df['label'].values  # Adjust if 'label' column has a different name\n",
        "class_weights = compute_class_weight(\"balanced\", classes=np.unique(labels), y=labels)\n",
        "class_weights = torch.tensor(class_weights, dtype=torch.float32).to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define a custom Trainer to include class weights in the loss function\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.get(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.get(\"logits\")\n",
        "\n",
        "        # Use weighted cross-entropy loss\n",
        "        loss_fct = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "# Set up training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=3,\n",
        "    weight_decay=0.01\n",
        ")\n",
        "\n",
        "# Initialize the custom Trainer with class weights\n",
        "trainer = WeightedTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train_dataset,\n",
        "    eval_dataset=tokenized_val_dataset\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T10:40:22.439037Z",
          "iopub.execute_input": "2024-11-13T10:40:22.440043Z",
          "iopub.status.idle": "2024-11-13T11:01:32.631065Z",
          "shell.execute_reply.started": "2024-11-13T10:40:22.439986Z",
          "shell.execute_reply": "2024-11-13T11:01:32.630109Z"
        },
        "id": "gtGC1qrXukp8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T11:01:32.634757Z",
          "iopub.execute_input": "2024-11-13T11:01:32.635136Z",
          "iopub.status.idle": "2024-11-13T11:02:12.766399Z",
          "shell.execute_reply.started": "2024-11-13T11:01:32.635104Z",
          "shell.execute_reply": "2024-11-13T11:02:12.765253Z"
        },
        "id": "VrJpX16_ukp8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Step 1: Predict on test data\n",
        "test_results = trainer.predict(eval_dataset)  # Replace test_dataset with your test data variable\n",
        "preds = np.argmax(test_results.predictions, axis=1)  # Get the predicted class labels\n",
        "labels = test_results.label_ids  # True labels from test data\n",
        "\n",
        "# Step 2: Calculate performance metrics\n",
        "accuracy = accuracy_score(labels, preds)\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')\n",
        "\n",
        "# Print out performance metrics\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "\n",
        "# Step 3: Generate Confusion Matrix\n",
        "conf_matrix = confusion_matrix(labels, preds)\n",
        "\n",
        "# Plot Confusion Matrix\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=label_mapping.keys(), yticklabels=label_mapping.keys())\n",
        "plt.xlabel(\"Predicted Label\")\n",
        "plt.ylabel(\"True Label\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T11:02:12.767712Z",
          "iopub.execute_input": "2024-11-13T11:02:12.768172Z",
          "iopub.status.idle": "2024-11-13T11:02:33.671414Z",
          "shell.execute_reply.started": "2024-11-13T11:02:12.768127Z",
          "shell.execute_reply": "2024-11-13T11:02:33.670447Z"
        },
        "id": "oWkzaFXuukp8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Generate the classification report\n",
        "report = classification_report(labels, preds, target_names=label_mapping.keys())\n",
        "\n",
        "# Print the classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(report)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T11:02:33.67273Z",
          "iopub.execute_input": "2024-11-13T11:02:33.673692Z",
          "iopub.status.idle": "2024-11-13T11:02:33.690053Z",
          "shell.execute_reply.started": "2024-11-13T11:02:33.673637Z",
          "shell.execute_reply": "2024-11-13T11:02:33.689194Z"
        },
        "id": "eDzTRwAgukp8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Assume 'model' and 'tokenizer' are already defined and loaded\n",
        "# Set the model to evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Move model to the same device as the inputs\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Function to predict sentiment\n",
        "def predict_sentiment(text):\n",
        "    # Tokenize the input text\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
        "\n",
        "    # Move inputs to the same device\n",
        "    for key in inputs:\n",
        "        inputs[key] = inputs[key].to(device)\n",
        "\n",
        "    # Disable gradient calculation for inference\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Print raw logits for insight\n",
        "    #print(\"Logits:\", outputs.logits)\n",
        "\n",
        "    # Get the predicted class\n",
        "    predictions = outputs.logits.argmax(dim=-1)\n",
        "\n",
        "    # Map the predicted class index to your labels (adjust this based on your label mapping)\n",
        "    label_mapping = {0: 'sadness', 1: 'joy', 2: 'love', 3: 'anger', 4: 'fear', 5: 'surprise'}\n",
        "    predicted_label = label_mapping[predictions.item()]\n",
        "\n",
        "    return predicted_label\n",
        "\n",
        "# Example usage\n",
        "test_sentences = [\n",
        "    \"I am so sad and alone.\",  # Expected: sadness\n",
        "    \"This is the best day ever!\",  # Expected: joy\n",
        "    \"I love this place!\",  # Expected: love\n",
        "    \"I am angry at the situation.\",  # Expected: anger\n",
        "    \"I fear that we will lose.\",  # Expected: fear\n",
        "    \"What a surprising turn of events!\"  # Expected: surprise\n",
        "]\n",
        "\n",
        "for text in test_sentences:\n",
        "    predicted_sentiment = predict_sentiment(text)\n",
        "    print(f\"Input: '{text}' -> Predicted Sentiment: {predicted_sentiment}\")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2024-11-13T11:02:33.691266Z",
          "iopub.execute_input": "2024-11-13T11:02:33.692019Z",
          "iopub.status.idle": "2024-11-13T11:02:33.775221Z",
          "shell.execute_reply.started": "2024-11-13T11:02:33.691974Z",
          "shell.execute_reply": "2024-11-13T11:02:33.77435Z"
        },
        "id": "HpxN4rMeukp8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}